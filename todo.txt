TODO:

-> Aim is to have all coding completed (or near to it) by end of tomorrow
	-> Make easy ways to test the different methods
	-> Once methods have been fully tested, we need all the players in one program
	-> Set up the testing environment (data collection etc) so that we can run it on multiple computers
	-> Even if programs seem to be not as good as they could be (e.g. Implementation 1) you will still need
	   to build the testing environments and get the report started.

-> Begin write up of final report - we want first draft done in 2 weeks
	-> Get the different sections of the report outlined
	-> See what you can / can't include in the report
	-> Read through paper "an analysis of UCT in multiplayer games" - especially the Hearts section

-> Are we testing more than the three players? Are we including trials of the different methods of doing things?

-> Have a go at fixing the changes Micahel suggested in your implementation write up
-------------------------------------------------------------------------------

Current Goals:

-> Improving implementation 2
	-> Make it so that the Select method doesn't always choose a leaf node
	-> Ensure that the assign children is adding, not overwriting
	-> Maybe leave till later? Get other things up and running first, since this algorithm is working well.

-> Implement the BASIC PLAYER
	-> Just play highest card possible that will not win the trick?
	-> Compare basic player and advanced player
	-> Maybe make a bit worse? Performing similar to the Advanced player

-> Keep testing that results are good!
	-> Will need to go over implementation 1, it keeps returning worse values

-> IMPLEMENTATIONS HAVE A BIG FLAW!!!!! - if changing, do it SEPARATLY! Keep the old code!
	-> It will SELECT down to a leaf node, and then perform a simulation. This does not work in this implementation,
	   as trick records are not stored. So you are randomly removing heaps of information and only really simulating
	   out the final trick!
	-> Need to have some way to simulate at different nodes, not just leaf nodes!!
	-> FIND A FIX FOR THIS!!!!!! We are not collecting much information. Simulations need to be spread throughout the tree.
	-> Is this an issue for the other type? Maybe not as much, BUT STILL THINK ABOUT IT!!! We may only ever be expanding
	   at a non leaf node once... so we need to fix this!
	
-> Check that implementation 1 is only playing legal hands (actually print some code to test!)

-> Need to find another unit of measurement than 10seconds, because this varies wildly based between computers.
	-> Maybe number of separate MCTS performed?
	-> Ask Michael. If not using time you can also experiment with changing settings much easier.
		-> Could always see what 10 seconds gives to the algorithm, and then hard code that

-> TESTING
	-> Test the Controller.Generate... methods
	-> Test the expand method only expanding legal plays
	-> Expand has been "fixed" - needs testing! Are we including right cards? Expanding where we should? Not looping?

-> Getting the other methods sorted
	-> I think implementation 1 is fine, but keep looking at notes below...
	-> Expansion WILL need redone in previous implementations so they take into account cards already played!!!	
		-> And be sure you are always retaining the players hand!!!
		-> Test anywhere hands need to be generated. This bug is critical and needs a solution!!!
		-> Afterwards test your programs again for any improvements!!!

-> Build the Simulate Play method for the 2nd implementation
	-> Make sure scores are being applied correctly to nodes and things.

-> More DoubleDummy information in PlayOut method?

-> Expansion method:
	-> Assigning the children to the node needs to append (not replace)
		-> Will also need to check against duplicates
	-> Need to find a way to keep / generate a deal at the beginning of the MCTS
	-> Maybe do the actual expand method first
	-> Root node needs to be assigned the initial trick (could be empty or upto 3 cards)
		-> Trick needs to have filler null nodes when passed to MCTS search	
		-> Root node start player set to -1, so cannot have simulate play called on it

Notes: 
State records information about the game (e.g. what has been played)

The MCTS will record information about the deal it has chosen, since
this is specific to the MCTS. The methods will be given this information
when called. A deal will be assigned at the start and remove all cards
that have already been played, recorded in the state.

TRICKS RECORDED AS (P1, O1, O2, O3) AND THE STARTPLAYER STORED

Currently expansions are based on a simply random algorithm, the idea being that we randomly
generate possible moves and then test them via random simulation.

-------------------------------------------------------------------------------

List of things to do on the hearts game:	

1) Investigate whether there is a bias for order of Hearts players
	-> There should be no bias at all, all spots should perform equally well

2) Do some checks on the backpropagation to ensure that the scores are correct.
	-> Remeber that a node has its own score as well as the score of the nodes below it.

6) REDO THE APP FOR TESTING PURPOSES
	-> Should be testing all values on same hands
	-> Should be recording results so you can find SD
	-> Could be useful to practice getting these results into excel
	-> If you build a prototype results collector, you'll have less work to do later on

7) Test for coefficient and thresehold in UCT
	-> Talk to Michael about these results
	-> Write them up!

8) Tests on how long we should let the MCST develop for? e.g. value of timer

9) Continue to implement DoubleDummyModular
	-> Do we need to even do this? Maybe just comparing the different methods of Selection and Simulation are enough?
	-> Maybe come back and do it after third implementation?

10) Write up explanantion of implementations

-------------------------------------------------------------------------------

SCHEUDLE:

8th September - All implementations / coding done
15th September - First draft of final report submitted
22nd September - Second draft of final report submitted
3rd October - Final copy of report submitted

-------------------------------------------------------------------------------

NOTES ON TIMING AND INCONSISTENCY:

The program right now is inconsistent. This can be fixed in a couple of ways.

1) Increase timer for MCTS - this will also ensure UCT is implemented correctly.
	-> Each game can take upto 10 seconds, or perhaps even longer.
	-> For the current state of the first implementation (14/08), having the timer set to 200,000
	   will have the cards executing at a rate of 9.37 seconds per game.

2) Increase the amount of runs we use as a sample
	-> Should satisfy condition of 2SD/sqrt(n) = 0.2

3) Introduce diminishing returns to the MCTS - e.g. the less nodes it can explore, the less time it has
-------------------------------------------------------------------------------
REPORT NOTES:

Include a paragraph of some general findings / decisions regarding...
	-> UCT Parameters
		-> Coeefficient
		-> Thresehold
	-> Number of tests (found with SD)
	-> Amount of time given to the MCTS algorithm

Include in future work a discussion on the advantages of card counting / more soft information, since your focus was not on that.

-------------------------------------------------------------------------------

IDEAS FOR SECOND IMPLEMENTATION:

1) Currently, one MCTS is drawn and at each node a simulate play method is completed, each with a different random set of cards.
	-> Instead of having one tree, have multiple
	-> The tree assigns hands at the start, then traverses
	-> Caluclate the best trees as opposed to best nodes

2) Implement the UCT algorithm for the Select method.

3) Improve the DoubleDummy playout with more advanced logic
	-> E.g. play highest card unless winning trick
	-> Maybe try to get rid of one suit first

4) Add an option for the DoubleDummy simulation to have preset players as the logic
	-> Make modular

-------------------------------------------------------------------------------

IDEAS FOR THIRD IMPLEMENTATION:

1) Introduce more hard and soft information
	-> e.g. card counting and prediction

-------------------------------------------------------------------------------

SOME NOTES:

-------------------------------------------------------------------------------

IMPLEMENTATION 0: Current improvement over random 3-4%

SELECT METHOD:

Currently, the select method works in a very naive fashion.
For each node, we calculate the average score of that node,
and then we subtract this from 13 to get X. We then add
X + 1 of the node to an array.

Once we have done this for all nodes, we randomly select from
this array. This is a very simple approach for biasing nodes
with a low average score.

EXPANSION METHOD:

Creates child nodes based on the card that can be played at the node.
	-> Represents a PLAY, not the whole trick

SIMULATE METHOD:

Upon reaching a node to begin a random simulation...
	-> Remove previously played cards from hand
	-> Randomly remove the equivalent number of opponents cards from remaining deck
	-> Randomly deal hands to the opponents
	-> Begin a "DoubleDumy" playout under these conditions
		-> DoubleDummy very naive, based on some simple rules
		-> Will try avoid trick if Hearts, otherwise win
		-> Does not support half made tricks
		-> How significant will improvements be on this?
	-> Simple algorithm - if no Hearts, play highest card, else lowest
	-> Record results

BACKPROPAGATE METHOD:

Basic backprop, adding score and visit count to previous nodes.

-------------------------------------------------------------------------------

IMPLEMENTATION 1: Current improvement over random 9-10%

SELECT METHOD:
	-> UCT with chosen parameters

EXPANSION METHOD:

SIMULATE METHOD:

Number of improvements to the DoubleDummy playouts:
	-> Randomised the start player (when no current trick passed) rather than assuming the player would start
	-> Added support for halfway through trick
	-> Potential TODO: change how the algorithm responds to playing different players

BACKPROPAGATE METHOD:

-------------------------------------------------------------------------------

IMPLEMENTATION IDEAS:

* Make a hand represent a trick as opposed to just a hand

* Swap out the tree approach - instead of one tree with random deals at nodes, have multiple
  trees that define a permanent deal at the root. Calculate the best move from the most common
  move returned from the trees.

-------------------------------------------------------------------------------

MISC NOTES / OBSERVATIONS:

* Had to randomise the start player for low trick games
	-> It seems that the start player has a disadvantage, assumed to be
	   because other players could not follow suit.

* Simply improving the DoubleDummy playout method had a huge improvement over performance,
  suggesting that a great deal of the MCTS capability relies on the DoubleDummy method itself.

-------------------------------------------------------------------------------

EVALUATION METHODS

There are three main methods that are to be used in the evaluation of the algorithms.
Each of these methods assumes 6 player types.
	1) Random
	2) Basic Player (Rule based)
	3) Advanced Player (Rule based)
	4) MCTS Implementation 0
	5) MCTS Implementation 1
	6) MCTS Implementation 2

Also to note, we need to retain the most information possible - probably results of each game
copyed into a program like excel.

METHOD 1

Say we wish to compare our players.

We playout some number of games with random deals (say 1000). Each player plays against
3 other random opponents, and we look at the average number of Hearts collected. The same
hands / games need to be played out by all players (not a different random game for each player).

Once we have this information, we can then look at the difference in average Hearts collected
in a graph format.

We can then place the difference in a table like this...

	1	2	3
1	0.2	1	0.6 	...

2	1	2	0.8	...

3	3.4	5	0.7	...



METHOD 2:

From the above table, we may choose to look at some of the bigger differences in more detail.

We can then play a player against X Y and Z, where XYZ are players from our player pool, and we
can investigate how well they do against other player types.

METHOD 3:

When looking at MCTS specifically, we can make a graph of performance to timer varaible - as we increase
how much time we are giving the algorithm, our results should also improve.

OTHER METHODS:

We can consider other methods, for example comparing components of MCTS, as we wish. But we should start
with the main methods above.






















