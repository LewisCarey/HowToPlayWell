TODO:

-> Do your 412 assignment

-> Check out the datacom task

-> Get something done for Michael

-> Finish the first tournament environment
	-> Need to account for changing order of players (AdvancedPlayer and MCTS_two)

-> Implement BasicPlayer - maybe just avoid all cards?

-> REPORT
	-> While running tests on your programs, do some report stuff...
	-> 1) Fix up what Michael suggested in your implementations write up.
	-> 2) Add a write up for implementation 2
	-> 3) Fix your interim report
	-> 4) Put everything together into a final report doc
	-> 5) Write detailed notes / plans for other sections

-> Read through paper "an analysis of UCT in multiplayer games" - especially the Hearts section

-------------------------------------------------------------------------------

Current Goals:

-> Improving implementation 2
	-> Make it so that the Select method doesn't always choose a leaf node
	-> Ensure that the assign children is adding, not overwriting
	-> Maybe leave till later? Get other things up and running first, since this algorithm is working well.

-> Implement the BASIC PLAYER
	-> Just play highest card possible that will not win the trick?
	-> Compare basic player and advanced player
	-> Maybe make a bit worse? Performing similar to the Advanced player

Notes: 
State records information about the game (e.g. what has been played)

The MCTS will record information about the deal it has chosen, since
this is specific to the MCTS. The methods will be given this information
when called. A deal will be assigned at the start and remove all cards
that have already been played, recorded in the state.

TRICKS RECORDED AS (P1, O1, O2, O3) AND THE STARTPLAYER STORED

Currently expansions are based on a simply random algorithm, the idea being that we randomly
generate possible moves and then test them via random simulation.

-------------------------------------------------------------------------------

List of things to do on the hearts game:	

1) Investigate whether there is a bias for order of Hearts players
	-> There should be no bias at all, all spots should perform equally well

2) Do some checks on the backpropagation to ensure that the scores are correct.
	-> Remeber that a node has its own score as well as the score of the nodes below it.

6) REDO THE APP FOR TESTING PURPOSES
	-> Should be testing all values on same hands
	-> Should be recording results so you can find SD
	-> Could be useful to practice getting these results into excel
	-> If you build a prototype results collector, you'll have less work to do later on

7) Test for coefficient and thresehold in UCT
	-> Talk to Michael about these results
	-> Write them up!

8) Tests on how long we should let the MCST develop for? e.g. value of timer

9) Continue to implement DoubleDummyModular
	-> Do we need to even do this? Maybe just comparing the different methods of Selection and Simulation are enough?
	-> Maybe come back and do it after third implementation?

10) Write up explanantion of implementations

-------------------------------------------------------------------------------

SCHEUDLE:

8th September - All implementations / coding done
15th September - First draft of final report submitted
22nd September - Second draft of final report submitted
3rd October - Final copy of report submitted

-------------------------------------------------------------------------------

NOTES ON TIMING AND INCONSISTENCY:

The program right now is inconsistent. This can be fixed in a couple of ways.

1) Increase timer for MCTS - this will also ensure UCT is implemented correctly.
	-> Each game can take upto 10 seconds, or perhaps even longer.
	-> For the current state of the first implementation (14/08), having the timer set to 200,000
	   will have the cards executing at a rate of 9.37 seconds per game.

2) Increase the amount of runs we use as a sample
	-> Should satisfy condition of 2SD/sqrt(n) = 0.2

3) Introduce diminishing returns to the MCTS - e.g. the less nodes it can explore, the less time it has
-------------------------------------------------------------------------------
REPORT NOTES:

Include a paragraph of some general findings / decisions regarding...
	-> UCT Parameters
		-> Coeefficient
		-> Thresehold
	-> Number of tests (found with SD)
	-> Amount of time given to the MCTS algorithm

Include in future work a discussion on the advantages of card counting / more soft information, since your focus was not on that.

-------------------------------------------------------------------------------

IDEAS FOR SECOND IMPLEMENTATION:

1) Currently, one MCTS is drawn and at each node a simulate play method is completed, each with a different random set of cards.
	-> Instead of having one tree, have multiple
	-> The tree assigns hands at the start, then traverses
	-> Caluclate the best trees as opposed to best nodes

2) Implement the UCT algorithm for the Select method.

3) Improve the DoubleDummy playout with more advanced logic
	-> E.g. play highest card unless winning trick
	-> Maybe try to get rid of one suit first

4) Add an option for the DoubleDummy simulation to have preset players as the logic
	-> Make modular

-------------------------------------------------------------------------------

IDEAS FOR THIRD IMPLEMENTATION:

1) Introduce more hard and soft information
	-> e.g. card counting and prediction

-------------------------------------------------------------------------------

SOME NOTES:

-------------------------------------------------------------------------------

IMPLEMENTATION 0: Current improvement over random 3-4%

SELECT METHOD:

Currently, the select method works in a very naive fashion.
For each node, we calculate the average score of that node,
and then we subtract this from 13 to get X. We then add
X + 1 of the node to an array.

Once we have done this for all nodes, we randomly select from
this array. This is a very simple approach for biasing nodes
with a low average score.

EXPANSION METHOD:

Creates child nodes based on the card that can be played at the node.
	-> Represents a PLAY, not the whole trick

SIMULATE METHOD:

Upon reaching a node to begin a random simulation...
	-> Remove previously played cards from hand
	-> Randomly remove the equivalent number of opponents cards from remaining deck
	-> Randomly deal hands to the opponents
	-> Begin a "DoubleDumy" playout under these conditions
		-> DoubleDummy very naive, based on some simple rules
		-> Will try avoid trick if Hearts, otherwise win
		-> Does not support half made tricks
		-> How significant will improvements be on this?
	-> Simple algorithm - if no Hearts, play highest card, else lowest
	-> Record results

BACKPROPAGATE METHOD:

Basic backprop, adding score and visit count to previous nodes.

-------------------------------------------------------------------------------

IMPLEMENTATION 1: Current improvement over random 9-10%

SELECT METHOD:
	-> UCT with chosen parameters

EXPANSION METHOD:

SIMULATE METHOD:

Number of improvements to the DoubleDummy playouts:
	-> Randomised the start player (when no current trick passed) rather than assuming the player would start
	-> Added support for halfway through trick
	-> Potential TODO: change how the algorithm responds to playing different players

BACKPROPAGATE METHOD:

-------------------------------------------------------------------------------

IMPLEMENTATION IDEAS:

* Make a hand represent a trick as opposed to just a hand

* Swap out the tree approach - instead of one tree with random deals at nodes, have multiple
  trees that define a permanent deal at the root. Calculate the best move from the most common
  move returned from the trees.

-------------------------------------------------------------------------------

MISC NOTES / OBSERVATIONS:

* Had to randomise the start player for low trick games
	-> It seems that the start player has a disadvantage, assumed to be
	   because other players could not follow suit.

* Simply improving the DoubleDummy playout method had a huge improvement over performance,
  suggesting that a great deal of the MCTS capability relies on the DoubleDummy method itself.

-------------------------------------------------------------------------------

EVALUATION METHODS

There are three main methods that are to be used in the evaluation of the algorithms.
Each of these methods assumes 6 player types.
	1) Random
	2) Basic Player (Rule based)
	3) Advanced Player (Rule based)
	4) MCTS Implementation 0
	5) MCTS Implementation 1
	6) MCTS Implementation 2

Also to note, we need to retain the most information possible - probably results of each game
copyed into a program like excel.

METHOD 1

Say we wish to compare our players.

We playout some number of games with random deals (say 1000). Each player plays against
3 other random opponents, and we look at the average number of Hearts collected. The same
hands / games need to be played out by all players (not a different random game for each player).

Once we have this information, we can then look at the difference in average Hearts collected
in a graph format.

We can then place the difference in a table like this...

	1	2	3
1	0.2	1	0.6 	...

2	1	2	0.8	...

3	3.4	5	0.7	...



METHOD 2:

From the above table, we may choose to look at some of the bigger differences in more detail.

We can then play a player against X Y and Z, where XYZ are players from our player pool, and we
can investigate how well they do against other player types.

METHOD 3:

When looking at MCTS specifically, we can make a graph of performance to timer varaible - as we increase
how much time we are giving the algorithm, our results should also improve.

OTHER METHODS:

We can consider other methods, for example comparing components of MCTS, as we wish. But we should start
with the main methods above.






















